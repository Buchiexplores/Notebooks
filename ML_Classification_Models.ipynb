{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Classification_Models",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZ1amWdKcZ2uUXKkNHGa1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buchiexplores/Notebooks/blob/master/ML_Classification_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VFWEzHHrEX4",
        "colab_type": "text"
      },
      "source": [
        "##**MY NOTEBOOK ON PROTOTYPING MACHINE LEARNING CLASSIFICATION MODELS**\n",
        "Compiled by **Abuchi Godswill Okeke**\n",
        "\n",
        "Linkedin: https://www.linkedin.com/in/abuchi-okeke-67b48a105/\n",
        "\n",
        "e-mail: okekeag@gmail.com\n",
        "\n",
        "#Features:\n",
        "* Upload files from google drive or local device drive\n",
        "* Dimensional reduction and feature selection using PCA\n",
        "* Cross-Validation to find best estimators\n",
        "* Random forest classifier\n",
        "* K nearest neighbor classifier\n",
        "* Support vector classifier\n",
        "* Ensemble learning\n",
        "* Model evaluation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPu3l5HFSDJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Importing Libraries\n",
        "\n",
        "#Scikit-learn (machine learning)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "# Plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
        "\n",
        "# tells matplotlib to embed plots within the notebook\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "#Other libraries\n",
        "from google.colab import files\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g4-yJgeTMxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = 'add your file ID here'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('filename.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8nAe961ciEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upload from your local device drive\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0VdtFqUf2Xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#label_df = pd.read_csv(io.BytesIO(uploaded['Yclassification_label.csv']))\n",
        "# Dataset is n#ow stored in a Pandas Dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOFcNqE2vlcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set base directory here\n",
        "base_dir = '/content/';"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCUy_LzDvOF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_data = np.loadtxt(base_dir + 'bread_samples_classification.csv', delimiter=',');\n",
        "y_data = np.loadtxt(base_dir + 'bread_classification_label.csv');\n",
        "X_data.shape\n",
        "y_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl8fxthSGF7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wavenumbers = np.loadtxt(base_dir + 'bread_wavenumbers.csv', delimiter=',');\n",
        "wavenumbers.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pohG88slGaUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualize data\n",
        "plt.figure()\n",
        "plt.gca().invert_xaxis()\n",
        "plt.plot(np.transpose(wavenumbers)[1],X_data, '-')\n",
        "plt.xlabel('xlabel_name');\n",
        "plt.ylabel('ylabel_name')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUg_zM5TSzaD",
        "colab_type": "text"
      },
      "source": [
        "#**Principal component analysis (PCA)** \n",
        "PCA is a technique for feature extraction that uses orthogonal transformation to extract import features from the input\n",
        "variables — so it can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space. It tries to preserve the essential parts that have more variation of the data and remove the non-essential parts with fewer variation. source: https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_PmLKCSKbUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PCA Dimensional Reduction\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ug058sXKlGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize the feature matrix\n",
        "data_rescaled = StandardScaler().fit_transform(np.transpose(X_data))\n",
        "# scaler = MinMaxScaler(feature_range=[0, 1])\n",
        "# data_rescaled = scaler.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUpZ-Hd00xz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA().fit(data_rescaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKJM1i_oyQY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the Cumulative Summation of the Explained Variance\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Variance (%)') #for each component\n",
        "plt.title('Pulsar Dataset Explained Variance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z17kPtyOKqqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a PCA that will retain the % variance needed (choose n_components from the plot above)\n",
        "pca = PCA(n_components=20, whiten=True)\n",
        "\n",
        "#Conduct PCA\n",
        "data_pca = pca.fit_transform(data_rescaled)\n",
        "#wavenumbers_pca = pca.fit_transform(np.transpose(wavenumbers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-THyJ8TkKr3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Show results\n",
        "print('Original number of features:', data_rescaled.shape[1])\n",
        "print('Reduced number of features:', data_pca.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhj8ckJd20SQ",
        "colab_type": "text"
      },
      "source": [
        "#**Data Splitting**#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ0p2s28pBeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split data from PCA into train and test samples\n",
        "train_sample, test_sample, train_label, test_label = train_test_split(data_pca, y_data, random_state = 42, test_size=0.3)\n",
        "print(train_sample.shape)\n",
        "print(test_sample.shape)\n",
        "print(train_label.shape)\n",
        "print(test_label.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4m3n71vhxCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If you're not using PCA split the original dataset here 80% : 20%\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, random_state=42, test_size=0.2)\n",
        "#print(X_train.shape)\n",
        "#print(X_test.shape)\n",
        "#print(y_train.shape)\n",
        "#print(y_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSPV13USXACM",
        "colab_type": "text"
      },
      "source": [
        "#**RandonForestClassifier**\n",
        "Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance. Source: https://www.datacamp.com/community/tutorials/random-forests-classifier-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1OMfwy6tSTU",
        "colab_type": "text"
      },
      "source": [
        "Uncomment below to run cross-validation and find the best number of estimators for your classifier. You can switch between classifiers of your choice and tune other hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp4WfvpQGbTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Cross-validation for RandomForestClassifier\n",
        "# #find the best n_estimators\n",
        "\n",
        "# estimators = list(range(900, 3000, 100))\n",
        "# # # empty list that will hold cv scores\n",
        "# cv_scores = []\n",
        "# # perform 10-fold cross validation\n",
        "# for k in estimators:\n",
        "#     Rdf = RandomForestClassifier(n_estimators=k, criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
        "#     scores = cross_val_score(Rdf, train_sample, train_label, cv=5, scoring='accuracy')\n",
        "#     cv_scores.append(scores.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZJp1ldgGehH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Plot the CV-results to show misclassification errors\n",
        "# mse = [1 - x for x in cv_scores]\n",
        "\n",
        "# # determining best k\n",
        "# optimal_k = estimators[mse.index(min(mse))]\n",
        "# print(\"The optimal number of estimators is {}\".format(optimal_k))\n",
        "\n",
        "# # plot misclassification error vs k\n",
        "# plt.plot(estimators, mse)\n",
        "# plt.xlabel(\"Number of Estimators K\")\n",
        "# plt.ylabel(\"Misclassification Error\")\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GY5GHxiG0BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build your RandomForestClassifier models with the best number estimators and other tunned parameters\n",
        "clf = RandomForestClassifier(n_estimators= 50, criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=-1, random_state=2000, verbose=0, warm_start=False, class_weight=None)\n",
        "clf.fit(train_sample, train_label)\n",
        "\n",
        "predsRf = clf.predict(test_sample)\n",
        "# print(\"Accuracy:\", accuracy_score(ytest,preds))\n",
        "#print(clf.score(X_train, y_train))\n",
        "print(\"accuracy: {}\".format(accuracy_score(test_label, predsRf)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsKCkPWYuhz4",
        "colab_type": "text"
      },
      "source": [
        "#**K-Nearest Neighbor(KNN) Classification**\n",
        "In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the one closest point to P1 and then the label of the nearest point assigned to P1. Read more (Credit): https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iByhUPUpuvMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build your KNeighborsClassifier models with the best number estimators and other tunned parameters from CV\n",
        "#Distances: euclidean, manhattan, hamming, minkowski\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=6, weights='uniform', algorithm='auto', leaf_size=1, p=2, metric='manhattan', metric_params=None, n_jobs=-1)\n",
        "knn.fit(train_sample, train_label)\n",
        "predsKNN = knn.predict(test_sample)\n",
        "# df = pd.DataFrame(clf.predict(X_test), columns=['Label'])\n",
        "# df.index += 1 # \"upgrade\" to one-based indexing\n",
        "# df.to_csv('Abuchi_knn_submission.csv',index_label='ID',columns=['Label'])\n",
        "# clf.score(X_train,y_train)\n",
        "print(\"accuracy: {}\".format(accuracy_score(test_label, predsKNN)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jByE3EKq79ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# dct = DecisionTreeClassifier(criterion='gini', max_features='auto', splitter='best', max_depth=1000, min_samples_split=2)\n",
        "# dct.fit(train_sample,train_label)\n",
        "# predsTree = dct.predict(test_sample)\n",
        "# print(\"accuracy: {}\".format(accuracy_score(test_label, predsTree)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY-_Z8-DwUjJ",
        "colab_type": "text"
      },
      "source": [
        "#**Support Vector Machine (SVM)**\n",
        "\n",
        "SVM is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well. Read more (Credit): https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHs5U5lJ--5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build your SVClassifier models with the best tunned parameters from CV\n",
        "svc = SVC(C=2.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.1,random_state=None, decision_function_shape='ovo')\n",
        "svc.fit(train_sample,train_label)\n",
        "predsSVC = svc.predict(test_sample)\n",
        "print(\"accuracy: {}\".format(accuracy_score(test_label, predsSVC)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TvYxegwxTNs",
        "colab_type": "text"
      },
      "source": [
        "#**Ensemble Learning**\n",
        "This produces one optimal classifier or predictive model by combining different base models together. Read more (Credit): https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f\n",
        "\n",
        "**Method Used**: Max voting method\n",
        "\n",
        "The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n",
        "\n",
        "credit: https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WLI9LkHJH-v-",
        "colab": {}
      },
      "source": [
        "#Ensemble your classifiers\n",
        "\n",
        "model1 = clf\n",
        "model2 = knn\n",
        "model3 = svc\n",
        "model = VotingClassifier(estimators=[('rf', model1), ('kn', model2), ('sc', model3)], voting='hard')\n",
        "model.fit(train_sample,train_label)\n",
        "model.score(test_sample,test_label)\n",
        "\n",
        "preds_training = model.predict(train_sample)\n",
        "preds = model.predict(test_sample)\n",
        "\n",
        "print(\"accuracy: {}\".format(accuracy_score(train_label, preds_training)))\n",
        "print(\"accuracy: {}\".format(accuracy_score(test_label, preds)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHYg9WJJ0YNj",
        "colab_type": "text"
      },
      "source": [
        "#**Model Evaluation**\n",
        "\n",
        "Model evaluation defines how well a model tends to perform on future (unseen/out-of-sample) data.\n",
        "\n",
        "Read more:\n",
        "https://towardsdatascience.com/model-evaluation-techniques-for-classification-models-eac30092c38b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOp81_5ndV_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Non-normalized confusion matrix for training\n",
        "cm_train = confusion_matrix(train_label, preds_training)\n",
        "# print(cm)\n",
        "classes = [\"class 1\",\"class 2\"]\n",
        "pd.DataFrame(cm_train, classes, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6f2e_2194A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Non-normalized confusion matrix\n",
        "cm_test = confusion_matrix(test_label, preds)\n",
        "# print(cm)\n",
        "classes = [\"class 1\",\"class 2\"]\n",
        "pd.DataFrame(cm_test, classes, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6gHcK_41-zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalized confusion matrix\n",
        "import seaborn as sns\n",
        "def norm_cm (cm):\n",
        "  cmn = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
        "  fig,ax = plt.subplots(figsize=(12,12))\n",
        "  sns_plot=sns.heatmap(cmn,annot=True,fmt='.2f')\n",
        "  plt.ylabel('Actual')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.show(block=False)\n",
        "  fig = sns_plot.get_figure()\n",
        "  fig.savefig(\"output.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcifAIXUeBGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualized Normalized Confusion Matrix for Train Tata\n",
        "norm_cm(cm_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evaMAHAJeLVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualized Normalized Confusion Matrix for Test Data\n",
        "norm_cm(cm_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zql-qut32QTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate evaluation parameters\n",
        "def eval_cm_parameters(cm):\n",
        "  np.seterr(divide='ignore', invalid='ignore')\n",
        "  FP = cm.sum(axis=0) - np.diag(cm)  \n",
        "  FN = cm.sum(axis=1) - np.diag(cm)\n",
        "  TP = np.diag(cm)\n",
        "  TN = cm.sum() - (FP + FN + TP)\n",
        "\n",
        "  FP = FP.astype(float)\n",
        "  FN = FN.astype(float)\n",
        "  TP = TP.astype(float)\n",
        "  TN = TN.astype(float)\n",
        "  \n",
        "  # Sensitivity, hit rate, recall, or true positive rate\n",
        "  TPR = TP/(TP+FN)\n",
        "  # Specificity or true negative rate\n",
        "  TNR = TN/(TN+FP) \n",
        "  # Precision or positive predictive value\n",
        "  PPV = TP/(TP+FP)\n",
        "  # Negative predictive value\n",
        "  NPV = TN/(TN+FN)\n",
        "  # Fall out or false positive rate\n",
        "  FPR = FP/(FP+TN)\n",
        "  # False negative rate\n",
        "  FNR = FN/(TP+FN)\n",
        "  # False discovery rate\n",
        "  FDR = FP/(TP+FP)\n",
        "  # Overall accuracy\n",
        "  ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "\n",
        "  metric_list = [\"FP\", \"TP\", \"FN\",\"TN\", \"TPR\", \"TNR\", \"PPV\", \"NPV\", \"FPR\", \"FNR\", \"FDR\", \"ACC\"]\n",
        "  metric_data = [FP, TP, FN, TN, TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC]\n",
        "  classes = [\"class 1\",\"class 2\"]\n",
        "  #pd.DataFrame(metric_data, metric_list, classes)\n",
        "  return metric_data, metric_list, classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOHCZX8ofMDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate confusion matrix parameters for train data\n",
        "metric_data, metric_list, classes = eval_cm_parameters(cm_train)\n",
        "pd.DataFrame(metric_data, metric_list, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpE4zJPKfa1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate confusion matrix parameters for test data\n",
        "metric_data, metric_list, classes = eval_cm_parameters(cm_test)\n",
        "pd.DataFrame(metric_data, metric_list, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP1mUvgulUM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#More on evaluation\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "def eval_cm (label, preds):\n",
        "  precision, recall, fscore, support = score(label, preds)\n",
        "  data_score = [precision, recall, fscore, support]\n",
        "  scores_heading = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
        "  classes = [\"CF(100%)\", \"CF + WF\"]\n",
        "  return data_score, scores_heading , classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ext4urOThVpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#More on evaluation for train\n",
        "data_score, scores_heading , classes = eval_cm (train_label, preds_training)\n",
        "pd.DataFrame(data_score, scores_heading , classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOlJYCVwg35I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#More on evaluation for test\n",
        "data_score, scores_heading , classes = eval_cm (test_label, preds)\n",
        "pd.DataFrame(data_score, scores_heading , classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EVyOE6l2w4Y",
        "colab_type": "text"
      },
      "source": [
        "**End of notebook**\n",
        "\n",
        "Please kindly reach out to me for contributions, addition or any other information.\n",
        "\n",
        "Sincerely,\n",
        "\n",
        "Abuchi"
      ]
    }
  ]
}